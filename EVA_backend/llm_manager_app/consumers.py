# EVA_backend/llm_manager_app/consumers.py

import asyncio
import json
import uuid
import re  # æ–°å¢æ­£åˆ™åº“
from typing import Dict, Optional
from channels.generic.websocket import AsyncWebsocketConsumer
from logs.logs import logger
from llm_manager_app.utils.llm_service import llm_service, Message
from memory_service_app.utils.redis_client import set_key, get_key  # âœ… å¼•å…¥ Redis å®¢æˆ·ç«¯æ–¹æ³•
from master_evolution.user_info_manager import user_info_manager
import logging
import time

class LLMConsumer(AsyncWebsocketConsumer):
    def __init__(self, *args, **kwargs):
        logging.debug("ã€DEBUGã€‘LLMConsumer.__init__ è¢«è°ƒç”¨")
        super().__init__(*args, **kwargs)
        self.llm_service = llm_service
        self.group_name = "llm_group"
        logger.info("ğŸ”„ LLMConsumer åˆå§‹åŒ–å®Œæˆ")

    async def connect(self):
        """å»ºç«‹ WebSocket è¿æ¥"""
        try:
            # åŠ å…¥ llm_group ç»„
            logger.info("ğŸ”„ å‡†å¤‡åŠ å…¥ llm_group...")
            if not self.channel_layer:
                raise RuntimeError("channel_layer æœªåˆå§‹åŒ–")
            await self.channel_layer.group_add(self.group_name, self.channel_name)
            logger.info(f"âœ… å·²åŠ å…¥ llm_group: {self.channel_name}")
            
            # æ¥å—è¿æ¥
            await self.accept()
            logger.info(f"ğŸŸ¢ LLMConsumer è¿æ¥æˆåŠŸ: {self.channel_name}")
            
            # å‘é€è¿æ¥æˆåŠŸæ¶ˆæ¯ï¼Œtypeç»Ÿä¸€ä¸ºsystem
            await self.send(text_data=json.dumps({
                "type": "system",
                "message": "LLMConsumer å·²è¿æ¥"
            }))
            
        except Exception as e:
            logger.error(f"âŒ LLMConsumer è¿æ¥å¤±è´¥: {str(e)}", exc_info=True)
            raise

    async def disconnect(self, close_code):
        """æ–­å¼€ WebSocket è¿æ¥"""
        try:
            # ç¦»å¼€ llm_group ç»„
            logger.info("ğŸ”„ å‡†å¤‡ç¦»å¼€ llm_group...")
            if not self.channel_layer:
                raise RuntimeError("channel_layer æœªåˆå§‹åŒ–")
            await self.channel_layer.group_discard(self.group_name, self.channel_name)
            logger.info(f"âœ… å·²ç¦»å¼€ llm_group: {self.channel_name}")
            logger.info(f"ğŸ”´ LLMConsumer æ–­å¼€è¿æ¥: {close_code}")
        except Exception as e:
            logger.error(f"âŒ LLMConsumer æ–­å¼€è¿æ¥å¤±è´¥: {str(e)}", exc_info=True)

    async def receive(self, text_data):
        """æ¥æ”¶å‰ç«¯ WebSocket æ¶ˆæ¯"""
        logging.debug(f"ã€DEBUGã€‘LLMConsumer.receive è¢«è°ƒç”¨ï¼Œtext_data: {text_data}")
        try:
            logger.info(f"ğŸ“¥ æ”¶åˆ°åŸå§‹æ¶ˆæ¯: {text_data}")
            data = json.loads(text_data)
            message_type = data.get("type")

            # å¤„ç†å¿ƒè·³æ¶ˆæ¯
            if message_type == "heartbeat":
                logger.debug("ğŸ’“ æ”¶åˆ° WebSocket å¿ƒè·³")
                await self.send(text_data=json.dumps({"type": "pong"}))
                return

            # å¤„ç†ç”¨æˆ·æ¶ˆæ¯
            if message_type == "message":
                message_id = data.get("message_id", str(uuid.uuid4()))
                user_message = data.get("message", "").strip()
                api_choice = data.get("api_choice", "deepseek")  # é»˜è®¤ä½¿ç”¨ deepseek
                need_speech = data.get("need_speech", False)
                voice_index = data.get("voice_index", 0)

                if not user_message:
                    logger.warning("âš ï¸ æ— æ•ˆçš„ LLM è¯·æ±‚: æ¶ˆæ¯ä¸ºç©º")
                    await self.send(text_data=json.dumps({
                        "type": "error",
                        "message": "æ¶ˆæ¯ä¸èƒ½ä¸ºç©º"
                    }))
                    return

                logger.info(f"ğŸ“© æ”¶åˆ°ç”¨æˆ·è¾“å…¥[{message_id}]: {user_message}")

                try:
                    # æå–å¹¶ä¿å­˜ç”¨æˆ·ä¿¡æ¯(æ–°å¢)
                    extracted_info = await user_info_manager.extract_and_save_user_info(user_message)
                    if extracted_info:
                        logger.info(f"âœ… ä»ç”¨æˆ·æ¶ˆæ¯ä¸­æå–åˆ°ä¿¡æ¯: {extracted_info}")

                    # å°†ç”¨æˆ·æ¶ˆæ¯å­˜å…¥ Redis
                    logger.info(f"ğŸ’¾ å¼€å§‹å­˜å‚¨ç”¨æˆ·æ¶ˆæ¯åˆ° Redis (message_id={message_id})")
                    await set_key(f"user_msg:{message_id}", user_message)
                    logger.info(f"âœ… ç”¨æˆ·æ¶ˆæ¯å·²å­˜å…¥ Redis (message_id={message_id})")

                    # å‘ memory_group å‘é€æ¶ˆæ¯ï¼Œè¯·æ±‚ç›¸å…³è®°å¿†
                    logging.debug(f"ã€DEBUGã€‘LLMConsumer.group_send memory_group: message_id={message_id}, user_message={user_message}")
                    await self.channel_layer.group_send(
                        "memory_group",
                        {
                            "type": "retrieve_memory",
                            "message_id": message_id,
                            "user_message": user_message
                        }
                    )
                    logger.info(f"âœ… è®°å¿†è¯·æ±‚å·²å‘é€åˆ° memory_group (message_id={message_id})")

                    # ç­‰å¾…è®°å¿†æ£€ç´¢å®Œæˆ
                    logging.debug(f"ã€DEBUGã€‘LLMConsumer._wait_for_memory è°ƒç”¨: message_id={message_id}")
                    final_context = await self._wait_for_memory(message_id)
                    logger.info(f"âœ… è®°å¿†æ£€ç´¢å®Œæˆ (message_id={message_id})")
                    
                    # ç”Ÿæˆå®Œæ•´å›ç­”
                    logger.info(f"ğŸ¤– å¼€å§‹ç”Ÿæˆ LLM å›ç­” (message_id={message_id})")
                    response = await self._generate_llm_response({
                        "user_message": user_message,
                        "final_context": final_context,
                        "api_choice": api_choice
                    })
                    logger.info(f"âœ… LLM å›ç­”ç”Ÿæˆå®Œæˆ (message_id={message_id})")

                    # === è‡ªåŠ¨è¿‡æ»¤æ‹¬å·å†…å®¹ï¼ˆåŠ¨ä½œ/è¡¨æƒ…/æ‹ŸäººåŒ–ï¼‰===
                    # è¿‡æ»¤æ‰€æœ‰ä¸­æ–‡/è‹±æ–‡æ‹¬å·å†…å†…å®¹
                    response = re.sub(r"[ï¼ˆ(][^ï¼‰)]*[ï¼‰)]", "", response)
                    # å»é™¤å¤šä½™ç©ºæ ¼
                    response = re.sub(r"\s+", " ", response).strip()
                    logger.info(f"âœ… å·²è¿‡æ»¤æ‹¬å·å†…å®¹åçš„å›å¤: {response}")

                    # å¦‚æœéœ€è¦è¯­éŸ³
                    logger.info(f"[TTS-DEBUG] need_speech={need_speech}, response={response}, voice_index={voice_index}")
                    speech_url = None
                    if need_speech:
                        try:
                            from speech.speech_manager import SpeechManager
                            speech_manager = SpeechManager()
                            logger.info(f"[TTS-DEBUG] SpeechManager åˆå§‹åŒ–æˆåŠŸï¼Œå‡†å¤‡ç”Ÿæˆè¯­éŸ³...")
                            speech_url = await speech_manager.generate_speech(
                                response,
                                voice_index=voice_index
                            )
                            logger.info(f"âœ… è¯­éŸ³ç”Ÿæˆå®Œæˆ: {speech_url}")
                        except Exception as e:
                            logger.error(f"âŒ è¯­éŸ³ç”Ÿæˆå¤±è´¥: {str(e)}", exc_info=True)
                            import traceback
                            logger.error(traceback.format_exc())
                    logger.info(f"[TTS-DEBUG] speech_url={speech_url}")
                    
                    logger.info(f"ğŸ“¤ å‡†å¤‡å‘é€å“åº”ç»™ç”¨æˆ· (message_id={message_id})")
                    await self.send(text_data=json.dumps({
                        "type": "response",
                        "message_id": message_id,
                        "response": response,
                        "context": final_context,
                        "speech_url": speech_url
                    }))
                    logger.info(f"âœ… å“åº”å·²å‘é€ç»™ç”¨æˆ· (message_id={message_id})")

                    # å°†å¯¹è¯å‘é€ç»™ MemoryConsumer ä¿å­˜åˆ°å·¥ä½œè®°å¿†
                    logging.debug(f"ã€DEBUGã€‘LLMConsumer.group_send memory_group: message_id={message_id}, user_message={user_message}, response={response}, final_context={final_context}")
                    await self.channel_layer.group_send(
                        "memory_group",
                        {
                            "type": "save_conversation",
                            "message_id": message_id,
                            "user_message": user_message,
                            "assistant_response": response,
                            "final_context": final_context
                        }
                    )
                    logger.info(f"âœ… å¯¹è¯å·²å‘é€åˆ° memory_group ç­‰å¾…ä¿å­˜ (message_id={message_id})")

                except Exception as e:
                    logger.error(f"âŒ ç”Ÿæˆå“åº”å¤±è´¥: {str(e)}", exc_info=True)
                    await self.send(text_data=json.dumps({
                        "type": "error",
                        "message": f"ç”Ÿæˆå“åº”å¤±è´¥: {str(e)}"
                    }))
            else:
                logger.warning(f"âš ï¸ æœªçŸ¥çš„æ¶ˆæ¯ç±»å‹: {message_type}")
                await self.send(text_data=json.dumps({
                    "type": "error",
                    "message": "æœªçŸ¥çš„æ¶ˆæ¯ç±»å‹"
                }))

        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSONè§£æå¤±è´¥: {str(e)}")
            await self.send(text_data=json.dumps({
                "type": "error",
                "message": "æ— æ•ˆçš„æ¶ˆæ¯æ ¼å¼"
            }))
        except Exception as e:
            logger.error(f"âŒ æ¶ˆæ¯å¤„ç†å¤±è´¥: {str(e)}", exc_info=True)
            await self.send(text_data=json.dumps({
                "type": "error",
                "message": f"æ¶ˆæ¯å¤„ç†å¤±è´¥: {str(e)}"
            }))

    async def _wait_for_memory(self, message_id: str, timeout: float = 5.0):
        """ç­‰å¾…è®°å¿†æ£€ç´¢å®Œæˆ"""
        logging.debug(f"ã€DEBUGã€‘LLMConsumer._wait_for_memory è¢«è°ƒç”¨ï¼Œmessage_id: {message_id}, timeout: {timeout}")
        try:
            memory_key = f"memory:{message_id}"
            logger.info(f"â³ å¼€å§‹ç­‰å¾…è®°å¿† (message_id={message_id})")
            start_time = time.time()
            while True:
                logger.debug(f"ğŸ”„ å°è¯•è·å–è®°å¿† (message_id={message_id})")
                memory = await get_key(memory_key)
                logger.info(f"[LLMConsumer] ä» Redis è¯»å–åˆ°çš„è®°å¿†å†…å®¹ (key={memory_key}): {memory}")
                if memory:
                    logger.info(f"âœ… æˆåŠŸè·å–è®°å¿† (message_id={message_id})")
                    return memory
                if time.time() - start_time > timeout:
                    logger.warning(f"â³ è®°å¿†æ£€ç´¢è¶…æ—¶ (message_id={message_id})")
                    return "è®°å¿†æ£€ç´¢è¶…æ—¶ï¼Œä½¿ç”¨é»˜è®¤ä¸Šä¸‹æ–‡ç»§ç»­ã€‚"
                logger.debug(f"â³ è®°å¿†å°šæœªå°±ç»ªï¼Œç»§ç»­ç­‰å¾… (message_id={message_id})")
                await asyncio.sleep(0.5)
        except Exception as e:
            logger.error(f"âŒ ç­‰å¾…è®°å¿†æ£€ç´¢å¤±è´¥: {str(e)}", exc_info=True)
            return "è®°å¿†æ£€ç´¢å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ä¸Šä¸‹æ–‡ç»§ç»­ã€‚"

    async def _generate_llm_response(self, data: Dict) -> str:
        """ç”ŸæˆLLMå›ç­”ï¼Œè‡ªåŠ¨è§£æfinal_contextå¹¶åˆ†å—æ³¨å…¥prompt"""
        try:
            user_message = data["user_message"]
            # è·å–ç”¨æˆ·ä¸ªæ€§åŒ–ä¿¡æ¯
            try:
                user_info = await user_info_manager.get_flat_user_info()
                if user_info:
                    logger.info(f"âœ… å·²è·å–ç”¨æˆ·æ‰å¹³åŒ–ä¿¡æ¯ç”¨äºä¸ªæ€§åŒ–å¯¹è¯ï¼š{list(user_info.keys())}")
            except Exception as e:
                logger.warning(f"âš ï¸ è·å–æ‰å¹³åŒ–ç”¨æˆ·ä¿¡æ¯å¤±è´¥ï¼Œå°†ä½¿ç”¨é»˜è®¤å¯¹è¯é£æ ¼: {str(e)}")
                user_info = {}

            # è§£æ final_contextï¼Œåˆ†å—æ³¨å…¥
            final_context = data.get("final_context", "")
            rules_block = ""
            conversation_block = ""
            memory_block = ""
            # ç”¨æ­£åˆ™åˆ†å—æå–
            import re
            rules_match = re.search(r"ã€ç³»ç»Ÿè§„åˆ™ã€‘([\s\S]*?)(?=ã€|$)", final_context)
            if rules_match:
                rules_block = rules_match.group(1).strip()
            conversation_match = re.search(r"ã€æœ€è¿‘å¯¹è¯ã€‘([\s\S]*?)(?=ã€|$)", final_context)
            if conversation_match:
                conversation_block = conversation_match.group(1).strip()
            memory_match = re.search(r"ã€ç›¸å…³è®°å¿†ã€‘([\s\S]*?)(?=ã€|$)", final_context)
            if memory_match:
                memory_block = memory_match.group(1).strip()

            # ç»„è£… prompt
            from prompts.prompt import build_eva_prompt
            context = await build_eva_prompt(
                user_message=user_message,
                conversation_history=conversation_block,
                memory_context=memory_block,
                user_info=user_info
            )
            # å¦‚æœ‰è§„åˆ™ï¼Œæ‹¼è¿› constraints
            if rules_block:
                context = f"ç³»ç»Ÿè§„åˆ™ï¼š\n{rules_block}\n\n" + context

            logger.info(f"[DEBUG] ä¼ é€’ç»™ LLM çš„ system prompt å†…å®¹: {context}")
            messages = [Message(role="user", content=user_message)]
            messages.insert(0, Message(role="system", content=context))
            api_choice = data.get("api_choice", "deepseek")
            self.llm_service.current_provider = api_choice
            logger.info(f"ğŸ”„ è®¾ç½®LLMæä¾›å•†: {api_choice}")
            logger.info(f"ğŸ“ å‘é€ç»™ LLM çš„ prompt: {[m.to_dict() for m in messages]}")
            response = await self.llm_service.generate(messages=messages)
            if isinstance(response, dict) and "content" in response:
                return response.get("content", "æš‚æ—¶æ— æ³•å›ç­”") 
            elif hasattr(response, "generations") and response.generations:
                return response.generations[0].message.content
            else:
                logger.warning(f"âš ï¸ æœªçŸ¥çš„å“åº”æ ¼å¼: {type(response)}")
                return "æš‚æ—¶æ— æ³•å›ç­”(å“åº”æ ¼å¼é”™è¯¯)"
        except Exception as e:
            logger.error(f"âŒ ç”ŸæˆLLMå›ç­”å¤±è´¥: {str(e)}", exc_info=True)
            raise

    # å¤„ç† memory_group è¿”å›çš„è®°å¿†æ£€ç´¢ç»“æœ
    async def process_memory_response(self, event):
        """
        å¤„ç†æ¥è‡ª memory_group çš„è®°å¿†æ£€ç´¢ç»“æœ
        """
        logging.debug(f"ã€DEBUGã€‘LLMConsumer.process_memory_response è¢«è°ƒç”¨ï¼Œevent: {event}")
        logger.info(f"ğŸ“¥ æ”¶åˆ° memory_group çš„è®°å¿†æ£€ç´¢ç»“æœ: {event}")
        # è¿™é‡Œå¯ä»¥æ ¹æ®å®é™…ä¸šåŠ¡éœ€æ±‚å¤„ç† event å†…å®¹
        await self.send(text_data=json.dumps({
            "type": "memory_response",
            "final_context": event.get("final_context", ""),
            "message_id": event.get("message_id", "")
        }))
        
    # å¤„ç† memory_ready æ¶ˆæ¯
    async def memory_ready(self, event):
        """
        å¤„ç†æ¥è‡ª memory_group çš„ memory_ready æ¶ˆæ¯
        åœ¨ MemoryConsumer ä¸­é€šè¿‡ memory_ready æ¶ˆæ¯é€šçŸ¥ LLMConsumer è®°å¿†å·²å‡†å¤‡å¥½
        """
        logging.debug(f"ã€DEBUGã€‘LLMConsumer.memory_ready è¢«è°ƒç”¨ï¼Œevent: {event}")
        logger.info(f"ğŸ“¥ æ”¶åˆ° memory_ready æ¶ˆæ¯: {event}")
        message_id = event.get("message_id")
        final_context = event.get("final_context")
        
        # å°†è®°å¿†å†…å®¹å†™å…¥Redis
        memory_key = f"memory:{message_id}"
        await set_key(memory_key, final_context, ex=3600)
        
        logger.info(f"âœ… è®°å¿†å·²å‡†å¤‡å°±ç»ª (message_id={message_id})")
        
        # å¯ä»¥é€‰æ‹©é€šçŸ¥å‰ç«¯è®°å¿†å·²å°±ç»ª
        # await self.send(text_data=json.dumps({
        #     "type": "memory_ready",
        #     "message_id": message_id
        # }))

    async def system_message(self, event):
        """å¤„ç†åç«¯æ¨é€çš„ç³»ç»Ÿé€šçŸ¥ï¼Œç›´æ¥è½¬å‘åˆ°å‰ç«¯èŠå¤©çª—å£ï¼ˆå¦‚çº¢è‰²ç³»ç»Ÿæ¶ˆæ¯ï¼‰"""
        await self.send(text_data=json.dumps({
            "type": "system",
            "message": event.get("message", "ç³»ç»Ÿé€šçŸ¥"),
            "level": event.get("level", "info")
        }))
